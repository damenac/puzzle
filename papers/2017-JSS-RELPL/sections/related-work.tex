\section{Related Work}
\label{sec:relatedwork}

The idea of reverse engineering software product lines from product variants has been already studied in the literature. Besides, there are several approaches that address this issue for the case in which the product variants have been built using the clone-and-own approach \cite{LopezHerrejon:2015,Martinez:2015,Martinez:2015b}. Although the applicability of such idea to the specific case of language product lines is quite recent, there are some related work that we discuss in this section. 

\vspace{2mm}
\textbf{\textit{Recovering a language modular design.}} The first challenge during reverse engineering language of product lines is to recover a language modular design. Although this challenge has not received proper attention, we found an approach that proposes insightful advances in this direction \cite{Kuhn:2015}. In that work, the language modular design is achieved by defining one language module for each construct. That means that the reverse engineering process will result in a language product line containing as many features as constructs exist in the DSLs.

This approach permits to exploit the variability in the language product line since it provides a high level of granularity in the decomposition of language modules. Hence, language designers can make decisions with an important level of detail. However, the complexity of the product line might increase unnecessarily. From the point of view of language users, there are clusters of language constructs that always go together thus separation is not needed. For example, in our running on state machines, the concepts of \texttt{StateMachine}, \texttt{State}, and \texttt{Transition}, go always together since they correspond to a commonality of all the input DSLs. Separating these constructs in different features is not necessary in this case and this increases the complexity of the variability models. This can be a real issue if language designers decide to apply automatic analysis operations on those models.

Differently, in our approach we use the notion of specification clones and intersections in order to achieve a level of granularity that captures the variability existing in the DSL variants given in the input. This permits to identify those clusters of language constructs that go always together in the given variants. This decision simplifies the language product line in the sense that the amount of language modules is lower than in the approach by Kuhn et al., \cite{Kuhn:2015}. In doing so, we certainly reduce the possible variants that can be configured by the language product line. This issue can be considered as a threat to validity of our approach. 

\vspace{2mm}
\textbf{\textit{Synthesizing variability models.}} The synthesis of variability models has been largely studied in the literature. Some of those approaches have been adapted for the particular case of variability in the context of language product lines engineering. The approach presented in \cite{Vacchi:2014} proposes a search-based technique to find a features model that represents the variability existing in a set of language modules while optimizing an objective function. This approach uses an ontology that describes the domain concepts of the language product line. The second approach (presented in \cite{Kuhn:2015}) refines the former by removing the ontology. This improvement is motivated by the difficulty behind the construction of such ontology. Then, the authors propose to annotate the BNF-like grammar with certain information that is used to create a variability model. 

The aforementioned approaches support not only abstract syntax variability, but also concrete syntax and semantic variability. In the first case, the ontology can be used to identify all the existing syntactic and semantic variation points since it represents the domain from both the syntax and semantic point of view. In the second case, the annotations provide the expressiveness enough to address all these dimensions of the variability.  

There is, however, an important limitation in those approaches. Although at the modeling level, feature models have shown their capabilities to represent multi-dimensional variability and it has been validated for language product lines, there is not support for effectively reverse-engineering such multi-dimensional variability in the language product lines. Indeed, the solution provided by current approaches is to synthesize variability models where each feature capture both the abstract syntax of the language constructs and their semantics. Using this strategy, a language construct that has different semantics interpretations is represented as two language features. Those features have the same abstract syntax (a repeated definition of the specification) and their corresponding semantics. 

The problem with this strategy is that it couples abstract syntax variability with semantics variability, which limits multi-staged configuration. The scenario in which language designers configure only the abstract syntax, and final users configure their semantics is not supported since the configuration of the semantics depends also to configure a segment of the abstract syntax. 

We claim that, in order to facilitate multi-staged configuration, the abstract syntax variability should be defined separately from the semantic variability. The main contribution of our approach constitutes an answer to that claim. We use feature models to represent abstract syntax variability, and orthogonal variability models to represent semantics variability.  

\section{Discussion: Broadening the Spectrum}

The approach presented in this article is only useful when language designers follow the development scenario described in Section \ref{sec:thedevelopmentscenario} while using the technological space mentioned in Section \ref{sec:technologicalscope}. Along this paper we show how we can reduce maintenance costs and exploit variability if those conditions are fulfilled. In this section we open the broaden by discussing potential directions to support more diverse scenarios. 

\vspace{2mm}
\textit{Thinking outside the clone-and-own approach.} An important constraint of our approach is that it is scoped to DSLs that have been built through the clone-and-own approach. This fact permit to assume the existence of specification clones which is the backbone of our strategy for reverse engineering language modules. But... what if we have DSLs that are not necessarily built in those conditions? Suppose for example that we have as input a set of DSLs that share certain commonalities but that have been developed in different development teams. In that case, the probability of finding specification scenarios is quite reduced, and our approach will not be useful. How our strategies can be extended to deal with such a scenario?

The answer to that question relies on the definition of more complex comparison operators. As we deeply explain in Section \ref{sec:reverse-engineering}, the very first step of our reverse engineering strategy is to perform a static analysis of the given DSLs and apply two comparison in order to specify specification clones. If what we want is to find commonalities that are not necessarily materialized in specification clones but in "equivalent functionality", then we need to enhance the comparison operators in order to detect such as equivalences. 

Note the complexity behind the notion of "equivalent functionality". In the case of abstract syntax, two meta-classes might provide equivalent functionality by defining different language constructs e.g., using different names for the specification elements and even different relationships among them. In the case of the semantics, two different domain specific actions might provide equivalent functionality through different programs. We claim that further research is needed to establish this notion of equivalence thus supporting more diverse development scenarios. 

%\vspace{2mm}
%\textit{Thinking outside metamodels and domain specific actions.} Another issue to consider is the comparison of the DSLs specifications. In our case, we propose a comparison operator that is quite strict in the sense that it validates that all the specification elements are the same. This guarantees that the is actually copy paste and permits to do the division in a safe way. However, we can loose some reuse opportunities. For example, .... We claim that there is room for better developing the comparison operators. We can for example, consider issues such as .. subtyping, or even. 

%At the implementation level we provide an interface that facilitates this process. 

%\textit{On the purpose of the modular design} The reverse engineering process is guided by a clear purpose. In our case, the purpose is to reduce the maintenance costs of the DSLs. This is evidenced in the breaking down strategy which is mainly focused on removing the specification clones. A direct consequence of this is that we are able to support the variation points existing in the given set of DSLs. For example, we know that the is one language supporting AND and OR triggers. However, our approach is limited to the variation points existing in the input DSLs. If for example, we want to create a new DSL that contains only ANDTrigger, the produced language product line will not permit that. Indeed, we consider that there is room for other strategies more intended to exploit the variability. For example, in the approach presented in X the division is performed at the level of the language constructs. Then, each feature represents one construct. The limitation of this approach, however, is that the variability models might become too large and the configuration process might be tedious. Other solution can be the support for human intervention during the breaking down process. Another solution might be the optimization of some well-designed principles. Indeed, we have some preliminary experiments by using meta-heuristics to optimize high-cohesion and low-coupling. The partial results are promising but in that case the development scenarios are quite diverse. 

%It is wort mentioning that at implementation level of our approach is conceived in such a way that the modularization strategy can be easily replaced. Indeed, we provide the interface IBreaker that supposes the method break. It receives a set of DSLs and it returns the set of language modules. In doing so, we facilitate the experimentation with new strategies that can be more appropriated to a particular context. 

