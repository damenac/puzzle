\section{Specifying language product lines}
\label{sec:meta-langauges}

In this chapter we present a solution to the first research question of this work: \textit{how to specify a language product line?} To this end, we provide a language modularization approach that supports languages modular design as language modules composition. Besides, we provide a strategy based on feature models and orthogonal variability models to represent the variability existing withing a language product line. 

\subsection{Language modularization}

During the construction of a language product line, language designers should implement DSLs in the form of interdependent \textit{language modules} which materialize language features. Each language module, provides a set of language constructs, and a DSL is obtained from the composition of two or more language modules. 

Just as in components-based software development \cite{Tripakis:2003}, languages modularization supposes the existence of two properties: separability and composability. \textit{Separability} refers to the capability of designing and verifying language modules independently of other language modules it may require. Separability relies on the definition of interfaces specifying the interactions between language modules. In turn, \textit{composability} refers to the capability of integrate several language modules to produce a complete and functional DSL. Composability relies on the usage of the interfaces between language modules in such a way that they can interchange both control and information.

\subsubsection{Achieving separability through language interfaces} 

As aforementioned, we propose the use of language interfaces to achieve separability of language modules. In particular, we propose the classical required and provided interfaces explained below. 

\vspace{2mm}
\textit{Required interfaces.} The purpose of a required interface is to support independent development of language modules. In that sense, a required interface is a mechanism that allows language designers to declare the needs that a language module has towards other modules while assuming that their needs will be eventually fulfilled. Suppose for example the development of a language module for finite state machines. This language module needs some additional abstractions such as constraints to express guards in the transitions. Using a required interface, language designers can declare those needs as a set of required constructs (e.g., \textsl{Constraint}) and focus on the definition of the constructs which are proper to finite state machines (e.g., \textsl{State}, \textsl{Transition}, \textsl{Triggers}). 

As the reader might infer from the last paragraph, the needs of a language module can be materialized in the form of required constructs. In assuming so, the specification of a language module would be composed of a set of actual constructs, which are being implemented by the module; and a set of required constructs, which represent needs to other modules. This approach would result useful to support the modularization scenario called aggregation where the needs of language modules are entire constructs implemented in foreign modules. However, a finer level of granularity might be necessary to support other modularization scenarios such as extension where the needs are not necessarily entire constructs but finer elements such as properties or operations. The details of the use of the required interfaces in extension will be explained later in this chapter. 

Based on this reasoning, we propose a mechanism to enable the capability to distinguish whether a given language specification element (i.e., meta-class, property, operation, parameter, enumeration, etc) corresponds to an actual implementation or a required declaration. The proposed mechanism is an extension to the EMOF meta-language that introduces the notion of \textit{virtualization}. Using this extension, language designers can define virtual specification elements expressing needs of the module. Non-virtual specification elements are actual implementations of the language module. The required interface of a language module is the set of virtual elements it contains within its specification.

Fig. \ref{fig:fig-req-example-fig} illustrates the use of required interfaces through the example introduced before regarding a language module for finite state machines that requires a constraint language. In that example, the constructs proper the state machines are non-virtual elements since they correspond to actual implementation of such abstractions. In turn, the constraints to express the guards in the transitions are expressed as a virtual construct called \textsl{Constraint} that provides a virtual operation action called \textsl{eval()} which is used in the specification of the semantics of the meta-class \textsl{Transition}. 

\begin{figure}
  \centering\includegraphics[width=0.86\linewidth]{images/fig-req-example-fig.png}
  \caption{Example of the use of required interfaces}
  \label{fig:fig-req-example-fig}
\end{figure}

\vspace{2mm}
\textit{Provided interfaces.} In our approach, the purpose of provided interfaces is to enable information hiding in the modular development of DSLs. That is, to distinguish between the information that specifies the functionality offered by the language module from the information corresponding to the implementation details behind such functionality. Consider for example a language module that offers the capability to express and evaluate constraints. Using a providing interface, language designers can express the essential functionality of the module i.e., expression and evaluation of constraints; and hide the implementation details and auxiliary concepts needed to achieve such functionality e.g., context management.

To support the definition of provided interfaces, we propose to extend EMOF with the notion of \textit{module visibility}. This extension allows to classify a certain specification element as either \textsl{\textbf{public}} or \textsl{\textbf{private}} according to its nature. For example, a language designer can classify a meta-class as \textsl{\textbf{public}} meaning that it represents essential functionality of the module so can be used by external modules and it belongs to the provided interface. Naturally, if the meta-class is classified as \textsl{\textbf{private}} it cannot be used by external modules and it cannot be considered as part of the provided interface. Note that the notion of module visibility is different from the notion of visibility already defined in EMOF. The later is associated to  certain access constraints of model elements with respect to the package in which they are implemented.

Fig. \ref{fig:fig-prov-example-fig} illustrates the use of provided interfaces through the example introduced before regarding the constraints module. Since the main functionality of the module is to define and evaluate constraints, the meta-classes included in the provided interface (so those one defined as \textsl{\textbf{public}} in terms of module visibility) are: \textsl{ConstrainedProgram} and \textsl{Constraint} including the operations that implement their semantics.

\begin{figure}
  \centering\includegraphics[width=1\linewidth]{images/fig-prov-example-fig.png}
  \caption{Example of the use of provided interfaces}
  \label{fig:fig-prov-example-fig}
\end{figure}

\subsubsection{Language composition through interfaces binding} 

Now we need to define the way in which those interfaces interact each other at the moment of the composition. In doing so, it is important to conciliate two different (and potentially conflicting) issues. Firstly, safe composition of the involved language modules should be guaranteed; we need to check the compatibility between a providing and a requiring language module by verifying that the functionality offered by the former actually fulfills the needs of later. Secondly, there must be some place for substitutability; compatibility checking should offer certain flexibility that permits to perform composition despite some differences in their definitions. This is important because when language modules are development independently of each other, their interfaces and implementations not always match \cite{Gschwind:2012}.

To deal with the aforementioned issues, we propose an approach for compatibility checking. It is at the same time strict enough to guarantee safe composition, and flexible enough to permit substitutability under certain conditions. Concretely, we propose to extract both required and provided interfaces in the form of \textit{model types} \cite{Steel:2007}. The model type corresponding to the required interface contains the virtual specification elements of a language module whereas the model type corresponding to the provided interface the model type contains its public specification elements. The relationship between a model type and a language module is called \textsl{\textbf{implements}} and it is introduced by Degueule et al. \cite{Degueule:2015a}. 

In order to perform compatibility checking, we use the sub-typing relationship --introduced by Guy et al. \cite{Guy:2012}-- between the model types corresponding to the provided and the required interfaces. This relationship imposes certain constraints that guarantee safe composition while permitting some freedom degrees thus introducing some flexibility. In particular, under this definition of sub-typing the most obvious manner to guarantee safe composition is to check two conditions: (1) all the needs expressed in the requiring model type are furnished in the providing model type (\textbf{total} sub-typing); and (2) the two model types have exactly the same shape (\textbf{isomorphic} sub-typing). However, this definition of sub-typing also provides two dimensions of flexibility: \textbf{partial} sub-typing and \textbf{non-isomorphic} sub-typing. 

The main principle behind partial sub-typing is that not all the needs expressed in the required model type must be provided by the provided one. In that case, compatibility checking corresponds to verify that the sub-set of elements that match in the model types are compatible. Then, the result of the composition is a third language module with a resulting required interface that contains those needs that have not been satisfied by the providing language module. 

As an example suppose a language module for finite state machines that needs not only constraints for expressing guards in the transitions, but also action scripting constructs to express the behavior of the states. In such a case, a constraint module will fulfill the first need but not the second one. Thanks to partial sub-typing, we can perform compatibility checking only on the constructs associated to the constraints and, if they are compatible, then compose those language modules. The result will be a language module having the constructs for state machines and constraints but that still needs action scripting constructs defined in its required interface.

The principle behind isomorphic sub-typing is that the needs in the requiring interface are not always expressed exactly as the functionality offered by the providing module is expressed in the providing interface. For example, model type in Fig. \ref{fig:fig-req-example-fig} expresses the needs in terms of constraints of state machines through a class \textsl{Constraint} with an operation \textsl{eval()}. If we want to use OCL to satisfy these needs, then we will find that there is not a class constraint but \textsl{OclExpr}. Besides, the operational semantics might be implemented differently. In this case, we need an adapter that permit to find the correspondences among the elements of the model types. 

At the implementation level, in the current state of our approach we support partial sub-typing and some particular cases of non-isomorphic sub-typing. We still need some research to fully support non-isomorphic sub-typing in a general specially at the moment of the composition. 

\textit{Language modules composition.} Once the compatibility between two language modules is correctly checked, the next step is to compose the language modules to integrate their functionality i.e., the needs of the requiring module are fulfilled with the services offered by the provided one.  In our approach, this composition is performed in two phases. First, there is a matching process that identifies one-to-one matches between virtual and public elements from the required and provided interface respectively. This match can be identified automatically by comparing names and types of the elements (where applicable). However, the match can be also specified manually in the case of non-isomorphism.

Once the match is correctly established, the composition process continues with a merging algorithm that replaces virtual elements with public ones. That means also to replace all the possible references existing to the virtual element to point out to the corresponding public element. When the process is finished, we re-calculate both provided and required interfaces. The provided interface of the composition is re-calculated as the sum of the public elements of the two modules under composition. In turn, the required interface of the composition is re-calculated as the difference of the required interface of the required module minus the provided interface of the providing module. 

\subsection{Language variability management}

The challenge towards supporting the variability existing in a language product line is that such variability is multi dimensional. Because the specification of a DSL involves several implementation concerns, then there are several dimensions of variability i.e., abstract syntax variability, concrete syntax variability, and semantic variability \cite{Cengarle:2009,Gronniger:2011}. Abstract syntax variability refers to the capability of selecting the desired language constructs for a particular type of user. In many cases, constructs are grouped in \textit{language features} to facilitate the selection. Such grouping is motivated by the fact that selecting constructs can be difficult because a DSL usually has many constructs, so a coarser level of granularity is required. In turn, concrete syntax variability refers to the capability of supporting different representations for the same language construct. Finally, semantic variability refers to the capability of supporting different interpretations for the same language construct.

In this section we present a strategy to deal with this type of variability. As the same as our approach to language modularization, our approach to variability management is scoped to abstract syntax and semantics; concrete syntax --and hence, concrete syntax variability-- is not being considered in the solution. In doing so, we consider that a solution to represent abstract syntax variability and semantic variability should consider two main issues. Firstly, the definition of the semantics has a strong dependency to the definition of the abstract syntax --the domain-specific actions that implement the semantics of a DSL are weaved in the meta-classes defined in the abstract syntax--. Hence, these dimensions of variability are not isolated each other. Rather, the decisions made in the configuration of the abstract syntax variability impact the decisions that can be made in the configuration of the semantic variability. For example, there is a variation point for the case of state machines that proposes different ways to deal with conflicting transitions. However, conflicting transitions are only possible within hierarchical state machines. Hence, if the state machine DSL does not support hierarchical states, then it makes no sense to configure this semantic variation point. 

The second issue to consider at the moment of dealing with language variability management is that a semantic variation point might be transversal to several meta-classes. For example, there is a semantic variation point in hierarchical state machines that corresponds to decide if the state machine follows either the run-to-completion policy or if it supports simultaneous events \cite{Crane:2007}. In any case, the implementation of this semantics involves code in domain-specific actions for the \textsl{StateMachine} and \textsl{Region} meta-classes. Moreover, if the involve meta-classes are introduced by different language modules in the abstract syntax, then the semantic variation point depends of two features. Hence, the relationship between a feature in the abstract syntax and a semantic variation point is not necessarily one-to-one. 

We can find several approaches to support multi dimensional variability (e.g., \cite{Rosenmuller:2011}). Some of those approaches have been applied concretely to language product lines. In particular, they propose to model all the dimensions of variability through feature models and then to establish dependencies among them. However, the differ in the way in which the features are organized. For example, in some cases there is a unique variability model with one branch for each dimension of variability whereas there are other approaches that propose the use of one feature model for each dimension and then they use cross-tree constraints to represent the dependencies among the dimensions. 

In this article we propose a different approach to deal with language variability management. Concretely, we propose to combine the use of feature models with orthogonal variability models; feature models are used to model abstract syntax variability and orthogonal variability models are used to model semantic variability. Fig. \ref{fig:languages-variability-modeling} shows illustrates our approach. At the top of the figure, we find a feature model which each feature represents one language module. At the bottom, we have an orthogonal variability model that represents the semantic variation points existing among those language modules. As the reader might imagine, each of the variants of the variation points are associated not only to a particular module but also to the corresponding domain-specific actions associated to its implementation. 

\begin{figure}
  \centering\includegraphics[width=0.84\linewidth]{images/languages-variability-modeling-fig}
  \caption{Approach to represent multi-dimensional variability in language product lines}
  \label{fig:languages-variability-modeling}
\end{figure}

\vspace{2mm}
\textbf{Why orthogonal variability models?} An inevitable question that we need to answer at this point is: why we use orthogonal variability models instead of using feature models as proposed by current approaches? The answer to this questions is three-fold:

\vspace{2mm}
\textit{(1) The structure of orthogonal variability models is more appropriated.} As explained by Roos-Frantz et al. \cite{Roos-Frantz:2012}, feature models and orthogonal variability models are similar. However, they have some structural differences. One of those differences is that whereas a feature model is a tree that can have many levels, an orthogonal variability model Is set of trees each of which has two levels. Each tree represent one variability point and its children represent variants to that variation point. 

Semantic variation points are decisions with respect to a particular segment of the semantics of a language. Although those decisions can have some dependencies among them --a decision \textit{A} might force a decision \textit{B}-- they can hardly be organized in a hierarchy. Indeed, we conducted an experiment where we use feature models to represent semantic variation points, and we always obtained two-level trees: the first level corresponds to the name of the variation point and its children represent the possible decisions. This fact suggests that orthogonal variability models are more appropriated than feature models to represent semantic variability.  

\vspace{2mm}
\textit{(2) The meaning of orthogonal variability models is more appropriated.} According to \cite{Liebig:2013}, a language feature is a characteristic provided by the language which is visible to the final user. This definition can be associated abstract syntax variability and the use of feature models can be appropriated to represented it. All the approaches on language product lines engineering use feature models to this end showing that it is possible and appropriated. 

The case of the semantic variability is different, however. A semantic decision is not a characteristic of a language that we can select or discard. The semantic of a DSL should be always specified if the DSLs is intended to be executable. Rather, a semantic decision is more a variation point that can have different interpretations captured as variants. Note vocabulary fits better in the definitions provided by orthogonal variability models. More than features, we have variation points and variants, which also suggest that the use orthogonal variability models is more appropriate to represent semantic variability.

\vspace{2mm}
\textit{(3) Decoupling variability favors multi-staged configuration.} Multi-staged configuration is a need that might appear during the construction of a language product line. Hence, we need to provide some support to facilitate a process where different stakeholders take different decisions during the configuration phase. Of course, this can be done by modeling the variability in a single model and ask each stakeholder to complete it according to its responsibilities. However, this approach can be error-prone since each stakeholder will have to visualize (and probably understand) information regarding variation points which he/she has not to configure.

To deal with this complexity, we propose to separate the models in which the abstract syntax variability and the semantic variability are expressed. Suppose the scenario introduced in Fig. \ref{fig:languages-configuration-modeling} where the language designer is responsible to configure the abstract syntax variability whereas the language user is responsible to configure the semantics. In that case, the language designer starts the process by taking decisions corresponding to the abstract syntax variability. Using our approach, the language designer will only visualize the information (i.e., the language features) regarding abstract syntax variability.

When the language designer finishes its configuration process, the orthogonal variability models will be available so the final user can perform the configuration of the semantics. This orthogonal variability model will only include the variation points that are relevant to the features included in the configuration of the abstract syntax. Moreover, because each of the semantic variation points are represented separately in a different tree, then we can imagine a scenario where the language designer is able to configure not only the abstract syntax but also some semantic variation points, and then delegate to the final user only the decisions that he/she can take according to its knowledge. 

\begin{figure}
  \centering\includegraphics[width=1\linewidth]{images/languages-configuration-fig}
  \caption{Approach to support multi-staged configuration of language product lines}
  \label{fig:languages-configuration-modeling}
\end{figure}