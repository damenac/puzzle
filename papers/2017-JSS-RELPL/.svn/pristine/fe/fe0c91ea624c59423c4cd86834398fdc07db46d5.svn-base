\section{Related work}
\label{sec:relatedwork}

\section{Discussion: Broadening the spectrum}

The approach presented in this article is only useful when language designers follow the development scenario described in Section \ref{sec:thedevelopmentscenario} while using the technological space mentioned in Section \ref{sec:technologicalscope}. Along this paper we show how we can reduce maintenance costs and exploit variability if those conditions are fulfilled. In this section we open the broaden by discussing potential directions to support more diverse scenarios. 

\vspace{2mm}
\textit{Thinking outside Copy\&Paste\&Modify.} An important constraint of our approach is that it is scoped to DSLs that have been built through the \textit{Copy\&Paste\&Modify} pattern. This fact permit to assume the existence of specification clones which is the backbone of our strategy for reverse engineering language modules. But... what if we have DSLs that are not necessarily built in those conditions? Suppose for example that we have as input a set of DSLs that share certain commonalities but that have been developed in different development teams. In that case, the probability of finding specification scenarios is quite reduced, and our approach will not be useful. How our strategies can be extended to deal with such a scenario?

The answer to that question relies on the definition of more complex comparison operators. As we deeply explain in Section \ref{sec:reverse-engineering}, the very first step of our reverse engineering strategy is to perform a static analysis of the given DSLs and apply two comparison in order to specify specification clones. If what we want is to find commonalities that are not necessarily materialized in specification clones but in "equivalent functionality", then we need to enhance the comparison operators in order to detect such as equivalences. 

Note the complexity behind the notion of "equivalent functionality". In the case of abstract syntax, two meta-classes might provide equivalent functionality by defining different language constructs e.g., using different names for the specification elements and even different relationships among them. In the case of the semantics, two different domain specific actions might provide equivalent functionality through different programs. We claim that further research is needed to establish this notion of equivalence thus supporting more diverse development scenarios. 

%\vspace{2mm}
%\textit{Thinking outside metamodels and domain specific actions.} Another issue to consider is the comparison of the DSLs specifications. In our case, we propose a comparison operator that is quite strict in the sense that it validates that all the specification elements are the same. This guarantees that the is actually copy paste and permits to do the division in a safe way. However, we can loose some reuse opportunities. For example, .... We claim that there is room for better developing the comparison operators. We can for example, consider issues such as .. subtyping, or even. 

%At the implementation level we provide an interface that facilitates this process. 

%\textit{On the purpose of the modular design} The reverse engineering process is guided by a clear purpose. In our case, the purpose is to reduce the maintenance costs of the DSLs. This is evidenced in the breaking down strategy which is mainly focused on removing the specification clones. A direct consequence of this is that we are able to support the variation points existing in the given set of DSLs. For example, we know that the is one language supporting AND and OR triggers. However, our approach is limited to the variation points existing in the input DSLs. If for example, we want to create a new DSL that contains only ANDTrigger, the produced language product line will not permit that. Indeed, we consider that there is room for other strategies more intended to exploit the variability. For example, in the approach presented in X the division is performed at the level of the language constructs. Then, each feature represents one construct. The limitation of this approach, however, is that the variability models might become too large and the configuration process might be tedious. Other solution can be the support for human intervention during the breaking down process. Another solution might be the optimization of some well-designed principles. Indeed, we have some preliminary experiments by using meta-heuristics to optimize high-cohesion and low-coupling. The partial results are promising but in that case the development scenarios are quite diverse. 

%It is wort mentioning that at implementation level of our approach is conceived in such a way that the modularization strategy can be easily replaced. Indeed, we provide the interface IBreaker that supposes the method break. It receives a set of DSLs and it returns the set of language modules. In doing so, we facilitate the experimentation with new strategies that can be more appropriated to a particular context. 

